<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="Author" content="Mike Owen">
   <meta name="Description" content="Buiding Instructions for Spheral++">
   <title>Spheral++ Build Page</title>
</head>
<body text="#FFFFFF" bgcolor="#C7C3C7" link="#66FFFF" vlink="#3366FF" alink="#FF0000" background="background.gif">

<blockquote>
<pre>&nbsp;</pre>

<center><table COLS=2 WIDTH="100%" NOSAVE >
<tr NOSAVE>
<a href="http://sourceforge.net"> 
<IMG src="http://sourceforge.net/sflogo.php?group_id=4943&type=5" width="210"
     height="62" border="0" alt="SourceForge Logo"></A>
<td>
</td>
</tr>
</table></center>

<hr ALIGN=LEFT WIDTH="100%"></blockquote>

<center>
<h1>
&nbsp;<a href="index.html">Back to Spheral++ main page</a></h1></center>

<center>
<hr WIDTH="100%"><font color="#CCFFFF"><h1>Building Spheral++</h1></font></center>
<font color="#FFFFFF">
<hr>

<h2>Step 1: Download the Spheral++ source.</h2>
We do not have any official releases yet, so the recommended method to download
Spheral++ currently is via CVS.  The quick start instructions for syncing with
the current state of the project are:
<ol>

  <li> Make yourself a directory where you want all of your Spheral++ code to
  live.  In the following example I will assume that you have make a directory
  under $HOME/Spheral++</li>

  <li> cd into the directory you just made, and issue the following set of CVS
  commands to download the current Spheral++ code.<br><br>
  cvs -d:pserver:anonymous@cvs.Spheral.sourceforge.net:/cvsroot/spheral login<br>
  cvs -z3 -d:pserver:anonymous@cvs.Spheral.sourceforge.net:/cvsroot/spheral co src<br>
  cvs -d:pserver:anonymous@cvs.Spheral.sourceforge.net:/cvsroot/spheral logout<br>
  cd src/<br>
  cvs update -dP<br><br></li>

  <li> In general, once you have downloaded the source you can always update to
  the current state of the code by going into the topmost src/ director and
  issuing the "cvs update -dP" command.</li>

</ol>
<hr>

<h2>Step 2: Make sure you have the required software installed.</h2>

Spheral++ depends on a core set of software that you must have, and there are
many optional packages you may want access to for use with Spheral.  At minimum
you must have
<ol>

  <li>
  <a href="http://www.python.org">Python.</a>
  Python is the main interface to Spheral++, and is absolutely required!  The
  odds are your system already has Python installed, but even if so I still
  recommend downloading and building your own for use with Spheral++.  The
  reason for this is that when you want to install extensions to Python for use
  with Spheral++, the default installation process for these extensions assumes
  you have write access to the Python directories (which is not the case with
  the system Python!)  It's also useful to be able to use an up to date version
  of Python, and system installations are often behind the times.

  <li><a href="http://www.gnuplot.info">Gnuplot</a> Gnuplot is current the
  standard 2-D plotting utility used in many of the Spheral++ test and demo
  scripts.  If your system already has Gnuplot installed, the system version
  should be OK.
  </li>

  <li><a href="http://www.pfdubois.com/numpy/">Numeric extension for Python</a></li>

  <li><a href="http://gnuplot-py.sf.net">Python interface to Gnuplot</a></li>

</ol>

Building both Numeric Python and the Gnuplot Python interface are *really*
simple, in most cases you should just have to run "python setup.py install",
but make sure you use the version of Python that you intend to run Spheral++
under!

Additionally, if you want to run the parallel version of Spheral++, you will need
<ol>
  <li>
  <a href="http://www-unix.mcs.anl.gov/mpi/mpich/">MPI</a>
  Parallel Spheral++ uses the Message Passing Interface (MPI) to coordinate
  parallel runs, so MPI is a must.  The standard vendor MPI on most parallel
  machines is fine, but if you need to you can install the freely available
  MPICH distribution from this link.
  </li>

  <li>
  <a href="http://sourceforge.net/projects/pympi">pyMPI.</a>
  The MPI enhanced version of Python.  Follow the directions on this page to
  download and build pyMPI.  When run in a distributed parallel mode,
  Spheral++ assumes a python process per processor, and requires that MPI be
  initialized before entering any Spheral++ module.  The parallel MPI enabled
  version of Python provides this functionality, and provides an "mpi" module
  that allows Python scripts to perform distributed operations (gather/scatter,
  reduction across processors, etc.)  This is required to run Spheral++ in
  parallel!
  </li>

  <li>
  <a href="http://www-users.cs.umn.edu/~karypis/metis/parmetis/index.html">
  ParMETIS.</a>
  ParMETIS is a parallel graph partitioning library, which Spheral++ uses to
  automatically partition (or domain decompose) an SPH distribution of nodes
  across processors.  The build system for ParMETIS is rather simple if
  low-tech: you simply edit the provided makefile filling in the appropriate
  compiler
  and associated flags for your system.  The resulting libraries (libmetis.a and
  libparmetis.a) and associated header file (parmetis.h) need to then be moved
  someplace Spheral++ can find them, as described in the configure section for
  <a href "#parmetis">"--with-parmetis="</a> configure flag below.
  </li>
</ol>

<br><br><hr>

<h2>Step 3: Configure for compiling</h2> 
Thanks to Martin Casado, Spheral++ now has an autoconf build process.  In most
cases you should simply have to execute the following set of steps (in the
src/src directory):
<font color="pink">
<ol>
  <li> ./boot </li>
  <li> ./configure </li>
</ol>
</font color>
There are of course many options you can give to <font
color="pink">configure</font color>.  You can see these options by typing
"./configure --help".  The current set of options you may want to use include:
<ol>
  <li><font color="pink">--without-mpi</font color>
  Compile Spheral++ for a serial run.  Use this option if you don't want to
  use the parallel distributed version of Spheral++.
  </li>
  <li><font color="pink">--with-CXX=ARG</font color>
  Manually set C++ compiler to ARG.
  </li>
  <li><font color="pink">--with-CC=ARG</font color>
  Manually set C compiler to ARG.  Spheral++ does not currently use any C code,
  so this option actually has no effect.
  </li>
  <li><font color="pink">--with-LD=ARG</font color>
  Manually set linker to ARG.
  </li>
  <li><font color="pink">--with-python=ARG</font color>
  Use non-standard python (other than the default in your path).
  </li>
  <li><font color="pink">--with-opt=Val</font color>
  Set optimization level (0,1,2,3).  Note that on some platforms (notably IBM
  AIX we have found that compiling at high optimizations (>2) sometimes fails!
  </li>
  <li><font color="pink">--without-assert</font color>
  Turn off Design by Contract asserts.  Use this option if you are compiling for
  an optimized run, and don't want these internal consistency checks to slow down
  the run time performance.
  </li>
  <li><font color="pink">--with-timers</font color>
  Turn on the Timer class profiling.  This will generate a table of times
  used by different sections of the code during a run.
  </li>
  <li><font color="pink">--with-papi</font color>
  Activate PAPI for class profiling.  Must be used in conjunction with
  --with-timers, and produces more information about the run time performance.
  </li>
  <li><a NAME="parmetis"></a><font color="pink">--with-parmetis=DIR</font color>
  Indicates the base directory where the required ParMETIS files are stored.
  Under this directory should be the following structure:
  <ul>
    <li>DIR/include/parmetis.h</li>
    <li>DIR/lib/libmetis.a</li>
    <li>DIR/lib/libparmetis.a</li>
  </ul>
  </li>
  <li><font color="pink">--enable-postgres</font color>
  Enable support for PostgreSQL
  </li>
  <li><font color="pink">--with-postgres-programs=DIR</font color>
  Location of PostgreSQL executables.
  </li>
  <li><font color="pink">--with-postgres-includes=DIR</font color>
  Location of PostgreSQL includes.
  </li>
  <li><font color="pink">--with-postgres-libs=DIR</font color>
  location of PostgreSQL libraries.
  </li>
</ol>
This process presupposes you have Gnu autoconf installed on your system.  This
is probably true for most people, but if not please vist the <a
href="http://www.gnu.org/software/autoconf/autoconf.html">Gnu autoconf</a> site
and install autoconf.
<br>
Note -- if you are trying to compile on IBM AIX please consult the README.AIX
under src/.
<br><br><hr>

<h2>Step 4:  Compile the code!</h2>
At long last you should be ready to compile the code, so go into the "src/src"
directory and type "make".  The libraries automatically install under the
path specified in the <font color="pink">configure</font color> stage above.
Be warned, Spheral++ can severely exercise your compiler, so (particularly if
you are building optimized) it might take some time to build!
<br><br><hr>

<h2>Step 5:  Configure your environment.</h2>
In order for your python process to find the Spheral++ components, you must set
the environment variables <font color="pink">PYTHONPATH</font color>
and <font color="pink">LD_LIBRARY_PATH</font color> to where you have
installed the Spheral++ components.  (On AIX this variable is actually called
<font color="pink">LIBPATH</font color>.)  Currently the Spheral++ process is
hardwired to place the modules you build into a <font color="pink">lib/</font
color> directory in the directory where you installed the Spheral++ source
code.
<br><br><hr>

<h2>Step 5:  Test your build.</h2>
At this point you should have all the Spheral++ modules built.  To test this,
start up the Python you specified to build against (or pyMPI if you are running
the parallel version), and execute the command "import Spheral".  It should
import all the Spheral++ modules successfully.  If you then type "dir(Spheral)"
at the prompt, you should see a long list of the Spheral++ classes, looking
something like this: <br><br>

>>> import Spheral<br>
>>> dir(Spheral)<br>
['ArtificialViscosity1d', 'ArtificialViscosity2d', 'ArtificialViscosity3d', 'ArtificialViscosityList1d', 'ArtificialViscosityList2d', 'ArtificialViscosityList3d', 'AsphNodeList1d', 'AsphNodeList2d', 'AsphNodeList3d', 'AverageVelocityFilter1d', 'AverageVelocityFilter2d', 'AverageVelocityFilter3d', 'BSplineKernel1d', 'BSplineKernel2d', 'BSplineKernel3d', 'Boundary1d', 'Boundary2d', 'Boundary3d', 'CGSUnits', 'CheapSynchronousRK2Integrator1d', 'CheapSynchronousRK2Integrator2d', 'CheapSynchronousRK2Integrator3d', 'CheapVonNeumanViscosity1d', 'CheapVonNeumanViscosity2d', 'CheapVonNeumanViscosity3d', 'CosmologicalUnits', 'DataBase1d', 'DataBase2d', 'DataBase3d', 'DiffusionFilter1d', 'DiffusionFilter2d', 'DiffusionFilter3d', 'EigenStruct1d', 'EigenStruct2d', 'EigenStruct3d', 'EquationOfState1d', 'EquationOfState2d', 'EquationOfState3d', 'FakeHydro1d', 'FakeHydro2d', 'FakeHydro3d', 'FieldBase1d', 'FieldBase2d', 'FieldBase3d', 'FileIO1d', 'FileIO2d', 'FileIO3d', 'FlatFileIO1d', 'FlatFileIO2d', 'FlatFileIO3d', 'FluidNodeList1d', 'FluidNodeList2d', 'FluidNodeList3d', 'GammaLawGasCGS1d', 'GammaLawGasCGS2d', 'GammaLawGasCGS3d', 'GammaLawGasCosmological1d', 'GammaLawGasCosmological2d', 'GammaLawGasCosmological3d', 'GammaLawGasMKS1d', 'GammaLawGasMKS2d', 'GammaLawGasMKS3d', 'GaussianKernel1d', 'GaussianKernel2d', 'GaussianKernel3d', 'GenericHydro1d', 'GenericHydro2d', 'GenericHydro3d', 'GridCellIndex1d', 'GridCellIndex2d', 'GridCellIndex3d', 'GridCellPlane1d', 'GridCellPlane2d', 'GridCellPlane3d', 'Hydro1d', 'Hydro2d', 'Hydro3d', 'Integrator1d', 'Integrator2d', 'Integrator3d', 'MKSUnits', 'MonaghanGingoldSumViscosity1d', 'MonaghanGingoldSumViscosity2d', 'MonaghanGingoldSumViscosity3d', 'MonaghanGingoldViscosity1d', 'MonaghanGingoldViscosity2d', 'MonaghanGingoldViscosity3d', 'Neighbor1d', 'Neighbor2d', 'Neighbor3d', 'NestedGridNeighbor1d', 'NestedGridNeighbor2d', 'NestedGridNeighbor3d', 'NodeIDIterator1d', 'NodeIDIterator2d', 'NodeIDIterator3d', 'NodeList1d', 'NodeList2d', 'NodeList3d', 'PeriodicBoundary1d', 'PeriodicBoundary2d', 'PeriodicBoundary3d', 'Physics1d', 'Physics2d', 'Physics3d', 'PiGaussianKernel1d', 'PiGaussianKernel2d', 'PiGaussianKernel3d', 'PlanarBoundary1d', 'PlanarBoundary2d', 'PlanarBoundary3d', 'Plane1d', 'Plane2d', 'Plane3d', 'ReflectingBoundary1d', 'ReflectingBoundary2d', 'ReflectingBoundary3d', 'Restart1d', 'Restart2d', 'Restart3d', 'ScalarField1d', 'ScalarField2d', 'ScalarField3d', 'ScalarFieldList1d', 'ScalarFieldList2d', 'ScalarFieldList3d', 'SphNodeList1d', 'SphNodeList2d', 'SphNodeList3d', 'SphVarGradNodeList1d', 'SphVarGradNodeList2d', 'SphVarGradNodeList3d', 'SuperGaussianKernel1d', 'SuperGaussianKernel2d', 'SuperGaussianKernel3d', 'SymTensor1d', 'SymTensor2d', 'SymTensor3d', 'SymTensorField1d', 'SymTensorField2d', 'SymTensorField3d', 'SymTensorFieldList1d', 'SymTensorFieldList2d', 'SymTensorFieldList3d', 'SynchronousRK2Integrator1d', 'SynchronousRK2Integrator2d', 'SynchronousRK2Integrator3d', 'TableKernel1d', 'TableKernel2d', 'TableKernel3d', 'Tensor1d', 'Tensor2d', 'Tensor3d', 'TensorField1d', 'TensorField2d', 'TensorField3d', 'TensorFieldList1d', 'TensorFieldList2d', 'TensorFieldList3d', 'Vector1d', 'Vector2d', 'Vector3d', 'VectorField1d', 'VectorField2d', 'VectorField3d', 'VectorFieldList1d', 'VectorFieldList2d', 'VectorFieldList3d', 'VonNeumanViscosity1d', 'VonNeumanViscosity2d', 'VonNeumanViscosity3d', 'W4SplineKernel1d', 'W4SplineKernel2d', 'W4SplineKernel3d', '__doc__', '__file__', '__name__', 'vector_of_ArtificialViscosity1dPtr', 'vector_of_ArtificialViscosity2dPtr', 'vector_of_ArtificialViscosity3dPtr', 'vector_of_AsphNodeList1dPtr', 'vector_of_AsphNodeList2dPtr', 'vector_of_AsphNodeList3dPtr', 'vector_of_FluidNodeList1dPtr', 'vector_of_FluidNodeList2dPtr', 'vector_of_FluidNodeList3dPtr', 'vector_of_GridCellIndex1d', 'vector_of_GridCellIndex2d', 'vector_of_GridCellIndex3d', 'vector_of_NodeList1dPtr', 'vector_of_NodeList2dPtr', 'vector_of_NodeList3dPtr', 'vector_of_Physics1dPtr', 'vector_of_Physics2dPtr', 'vector_of_Physics3dPtr', 'vector_of_ScalarFieldList1d', 'vector_of_ScalarFieldList2d', 'vector_of_ScalarFieldList3d', 'vector_of_SphNodeList1dPtr', 'vector_of_SphNodeList2dPtr', 'vector_of_SphNodeList3dPtr', 'vector_of_SphVarGradNodeList1dPtr', 'vector_of_SphVarGradNodeList2dPtr', 'vector_of_SphVarGradNodeList3dPtr', 'vector_of_SymTensor1d', 'vector_of_SymTensor2d', 'vector_of_SymTensor3d', 'vector_of_SymTensorFieldList1d', 'vector_of_SymTensorFieldList2d', 'vector_of_SymTensorFieldList3d', 'vector_of_Tensor1d', 'vector_of_Tensor2d', 'vector_of_Tensor3d', 'vector_of_TensorFieldList1d', 'vector_of_TensorFieldList2d', 'vector_of_TensorFieldList3d', 'vector_of_Vector1d', 'vector_of_Vector2d', 'vector_of_Vector3d', 'vector_of_VectorFieldList1d', 'vector_of_VectorFieldList2d', 'vector_of_VectorFieldList3d', 'vector_of_double', 'vector_of_int']<br>
<br>
If this succeeds, congratulations!  You are ready to go on try some of the run time tests in "src/tests".
</font><br><br>


</body>
</html>
